# ğŸš€ Guide de Configuration EC2 pour CityFlow

Ce guide vous aide Ã  configurer et uploader les donnÃ©es sur votre instance EC2.

---

## ğŸ“‹ PrÃ©requis

1. Instance EC2 lancÃ©e (Amazon Linux 2 recommandÃ©)
2. AccÃ¨s SSH configurÃ©
3. Python 3.9+ installÃ©
4. Tables DynamoDB crÃ©Ã©es (voir section ci-dessous)

---

## ğŸ”§ Ã‰tape 1 : CrÃ©ation des Tables DynamoDB

Avant de lancer le traitement, vous devez crÃ©er les tables DynamoDB :

### Via AWS CLI :

```bash
# Table pour les mÃ©triques
aws dynamodb create-table \
    --table-name cityflow-metrics \
    --attribute-definitions \
        AttributeName=metric_type,AttributeType=S \
        AttributeName=date,AttributeType=S \
    --key-schema \
        AttributeName=metric_type,KeyType=HASH \
        AttributeName=date,KeyType=RANGE \
    --billing-mode PAY_PER_REQUEST \
    --region eu-west-3

# Table pour les rapports
aws dynamodb create-table \
    --table-name cityflow-reports \
    --attribute-definitions \
        AttributeName=report_id,AttributeType=S \
        AttributeName=date,AttributeType=S \
    --key-schema \
        AttributeName=report_id,KeyType=HASH \
        AttributeName=date,KeyType=RANGE \
    --billing-mode PAY_PER_REQUEST \
    --region eu-west-3
```

### Via Console AWS :
1. Allez dans DynamoDB â†’ Tables â†’ Create table
2. CrÃ©er `cityflow-metrics` :
   - Partition key: `metric_type` (String)
   - Sort key: `date` (String)
   - Billing mode: On-demand
3. CrÃ©er `cityflow-reports` :
   - Partition key: `report_id` (String)
   - Sort key: `date` (String)
   - Billing mode: On-demand

---

## ğŸ“ Ã‰tape 2 : PrÃ©parer les Fichiers en Local

Assurez-vous d'avoir tous les fichiers nÃ©cessaires :

```bash
# Structure attendue en local :
bucket-cityflow-paris-s3-raw/
â””â”€â”€ cityflow-raw/
    â””â”€â”€ raw/
        â”œâ”€â”€ batch/
        â”‚   â”œâ”€â”€ comptages-routiers-permanents.csv
        â”‚   â”œâ”€â”€ chantiers-perturbants-la-circulation.csv
        â”‚   â””â”€â”€ referentiel-geographique-pour-les-donnees-trafic-issues-des-capteurs-permanents.csv
        â””â”€â”€ api/
            â”œâ”€â”€ bikes/
            â”‚   â””â”€â”€ dt=2025-11-04/
            â”‚       â””â”€â”€ hour=02/
            â”‚           â””â”€â”€ *.jsonl
            â”œâ”€â”€ traffic/
            â”‚   â””â”€â”€ dt=2025-11-04/
            â”‚       â””â”€â”€ hour=02/
            â”‚           â””â”€â”€ *.jsonl
            â””â”€â”€ weather/
                â””â”€â”€ dt=2025-11-04/
                    â””â”€â”€ hour=02/
                        â””â”€â”€ *.jsonl
```

---

## ğŸ“¤ Ã‰tape 3 : Uploader les Fichiers sur EC2

### Option A : Via SCP (RecommandÃ© pour fichiers < 1GB)

```bash
# Se connecter Ã  votre EC2
export EC2_HOST="ec2-user@<votre-ip-ec2>"
export KEY_PATH="<chemin-vers-votre-cle.pem>"

# CrÃ©er la structure de dossiers sur EC2
ssh -i $KEY_PATH $EC2_HOST "mkdir -p /home/ec2-user/cityflow-project/data/{batch,api/bikes,api/traffic,api/weather}"

# Uploader les fichiers CSV (batch)
scp -i $KEY_PATH \
    bucket-cityflow-paris-s3-raw/cityflow-raw/raw/batch/*.csv \
    $EC2_HOST:/home/ec2-user/cityflow-project/data/batch/

# Uploader les fichiers API
scp -i $KEY_PATH -r \
    bucket-cityflow-paris-s3-raw/cityflow-raw/raw/api/bikes/dt=2025-11-04 \
    $EC2_HOST:/home/ec2-user/cityflow-project/data/api/bikes/

scp -i $KEY_PATH -r \
    bucket-cityflow-paris-s3-raw/cityflow-raw/raw/api/traffic/dt=2025-11-04 \
    $EC2_HOST:/home/ec2-user/cityflow-project/data/api/traffic/

scp -i $KEY_PATH -r \
    bucket-cityflow-paris-s3-raw/cityflow-raw/raw/api/weather/dt=2025-11-04 \
    $EC2_HOST:/home/ec2-user/cityflow-project/data/api/weather/
```

### Option B : Via S3 comme Stockage IntermÃ©diaire (Pour gros fichiers)

```bash
# 1. Uploader vers S3 depuis votre machine locale
aws s3 cp bucket-cityflow-paris-s3-raw/cityflow-raw/raw/batch/ \
    s3://bucket-cityflow-paris-s3-raw/cityflow-raw/raw/batch/ \
    --recursive

aws s3 cp bucket-cityflow-paris-s3-raw/cityflow-raw/raw/api/ \
    s3://bucket-cityflow-paris-s3-raw/cityflow-raw/raw/api/ \
    --recursive

# 2. Sur EC2, tÃ©lÃ©charger depuis S3
ssh -i $KEY_PATH $EC2_HOST << 'EOF'
cd /home/ec2-user/cityflow-project
mkdir -p data/{batch,api}

# TÃ©lÃ©charger les fichiers batch
aws s3 cp s3://bucket-cityflow-paris-s3-raw/cityflow-raw/raw/batch/ \
    data/batch/ --recursive

# TÃ©lÃ©charger les fichiers API
aws s3 cp s3://bucket-cityflow-paris-s3-raw/cityflow-raw/raw/api/ \
    data/api/ --recursive
EOF
```

### Option C : Script Automatique (Plus Facile)

Utilisez le script fourni `upload_data_to_ec2.sh` :

```bash
chmod +x upload_data_to_ec2.sh
./upload_data_to_ec2.sh <ip-ec2> <chemin-cle.pem>
```

---

## âš™ï¸ Ã‰tape 4 : Configurer l'Environnement sur EC2

```bash
# Se connecter Ã  EC2
ssh -i $KEY_PATH $EC2_HOST

# Aller dans le projet
cd /home/ec2-user/cityflow-project

# Copier le fichier .env pour EC2
cp env.ec2.example .env

# VÃ©rifier que les chemins sont corrects
cat .env

# Installer les dÃ©pendances (si pas dÃ©jÃ  fait)
python3 -m venv venv
source venv/bin/activate
pip install -r requirements.txt
```

---

## ğŸ” Ã‰tape 5 : Configurer les Permissions IAM

Votre instance EC2 doit avoir un rÃ´le IAM avec ces permissions :

```json
{
  "Version": "2012-10-17",
  "Statement": [
    {
      "Effect": "Allow",
      "Action": [
        "dynamodb:PutItem",
        "dynamodb:GetItem",
        "dynamodb:Query",
        "dynamodb:Scan",
        "dynamodb:UpdateItem"
      ],
      "Resource": [
        "arn:aws:dynamodb:eu-west-3:*:table/cityflow-metrics",
        "arn:aws:dynamodb:eu-west-3:*:table/cityflow-reports"
      ]
    },
    {
      "Effect": "Allow",
      "Action": [
        "s3:GetObject",
        "s3:PutObject",
        "s3:ListBucket"
      ],
      "Resource": [
        "arn:aws:s3:::bucket-cityflow-paris-s3-raw/*",
        "arn:aws:s3:::cityflow-reports-paris/*"
      ]
    }
  ]
}
```

---

## âœ… Ã‰tape 6 : VÃ©rifier l'Installation

Sur EC2, exÃ©cutez ces commandes pour vÃ©rifier :

```bash
# VÃ©rifier que les fichiers sont bien prÃ©sents
ls -lh /home/ec2-user/cityflow-project/data/batch/
ls -lh /home/ec2-user/cityflow-project/data/api/bikes/
ls -lh /home/ec2-user/cityflow-project/data/api/traffic/
ls -lh /home/ec2-user/cityflow-project/data/api/weather/

# VÃ©rifier boto3
python3 << 'EOF'
import boto3
print("âœ“ boto3 installÃ©:", boto3.__version__)

# Test DynamoDB
dynamodb = boto3.resource('dynamodb', region_name='eu-west-3')
table = dynamodb.Table('cityflow-metrics')
print("âœ“ Table DynamoDB accessible:", table.table_name)
EOF

# Tester la connexion Ã  la base de donnÃ©es
python3 << 'EOF'
from utils.database_factory import test_database_connection
test_database_connection()
EOF
```

---

## ğŸš€ Ã‰tape 7 : Lancer le Traitement

```bash
# Lancer le traitement complet
./run_processing.sh

# Ou manuellement
source venv/bin/activate
python3 main.py
```

---

## ğŸ” En Cas d'Erreur

### VÃ©rifier les logs
```bash
cat logs/processing_*.log | tail -100
```

### ProblÃ¨mes Courants

#### 1. **Erreur : "Table does not exist"**
â†’ CrÃ©ez les tables DynamoDB (voir Ã‰tape 1)

#### 2. **Erreur : "Access Denied"**
â†’ VÃ©rifiez que le rÃ´le IAM est bien attachÃ© Ã  l'EC2 (voir Ã‰tape 5)

#### 3. **Erreur : "No such file or directory"**
â†’ VÃ©rifiez que les fichiers sont bien uploadÃ©s dans `/home/ec2-user/cityflow-project/data/`

#### 4. **Erreur : "boto3 not found"**
```bash
source venv/bin/activate
pip install boto3
```

---

## ğŸ“Š Structure des DonnÃ©es Attendue sur EC2

```
/home/ec2-user/cityflow-project/
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ batch/
â”‚   â”‚   â”œâ”€â”€ comptages-routiers-permanents.csv
â”‚   â”‚   â”œâ”€â”€ chantiers-perturbants-la-circulation.csv
â”‚   â”‚   â””â”€â”€ referentiel-geographique-pour-les-donnees-trafic-issues-des-capteurs-permanents.csv
â”‚   â””â”€â”€ api/
â”‚       â”œâ”€â”€ bikes/
â”‚       â”‚   â””â”€â”€ dt=2025-11-04/
â”‚       â”‚       â””â”€â”€ hour=02/
â”‚       â”‚           â””â”€â”€ *.jsonl
â”‚       â”œâ”€â”€ traffic/
â”‚       â”‚   â””â”€â”€ dt=2025-11-04/
â”‚       â”‚       â””â”€â”€ hour=02/
â”‚       â”‚           â””â”€â”€ *.jsonl
â”‚       â””â”€â”€ weather/
â”‚           â””â”€â”€ dt=2025-11-04/
â”‚               â””â”€â”€ hour=02/
â”‚                   â””â”€â”€ *.jsonl
â”œâ”€â”€ output/
â”‚   â”œâ”€â”€ metrics/
â”‚   â”œâ”€â”€ reports/
â”‚   â””â”€â”€ processed/
â”œâ”€â”€ logs/
â””â”€â”€ .env (copiÃ© depuis env.ec2.example)
```

---

## ğŸ’¡ Conseils

1. **Compression des fichiers** : Pour accÃ©lÃ©rer l'upload, compressez d'abord :
   ```bash
   tar -czf data.tar.gz bucket-cityflow-paris-s3-raw/cityflow-raw/raw/
   scp -i $KEY_PATH data.tar.gz $EC2_HOST:/home/ec2-user/
   ssh -i $KEY_PATH $EC2_HOST "cd /home/ec2-user && tar -xzf data.tar.gz"
   ```

2. **Monitoring** : Surveillez l'utilisation CPU/RAM pendant le traitement :
   ```bash
   htop
   ```

3. **Logs en temps rÃ©el** :
   ```bash
   tail -f logs/processing_*.log
   ```

---

## ğŸ“ Support

En cas de problÃ¨me, vÃ©rifiez :
1. Les logs : `logs/processing_*.log`
2. La connexion DynamoDB : `python3 -c "from utils.database_factory import test_database_connection; test_database_connection()"`
3. Les permissions IAM : Console AWS â†’ EC2 â†’ Instance â†’ Security â†’ IAM Role

