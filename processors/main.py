"""
Point d'entr√©e principal - Orchestration compl√®te du traitement des donn√©es CityFlow Analytics
Ex√©cut√© dans AWS (Lambda/EC2) pour le preprocessing
"""

import sys
import os
from datetime import datetime
from pathlib import Path
from typing import Dict, Any, Optional

# Ajouter le r√©pertoire parent au PYTHONPATH
sys.path.insert(0, str(Path(__file__).parent.parent))

# Imports configuration
from config import settings

# Imports processeurs
from processors import (
    BikesProcessor, TrafficProcessor, WeatherProcessor,
    ComptagesProcessor, ChantiersProcessor, ReferentielProcessor
)

# Imports utilitaires (depuis processors/utils/)
from processors.utils.file_utils import (
    load_json, find_json_files, load_and_combine_json_files, find_csv_files
)

# Import services base de donn√©es (MongoDB ou DynamoDB)
import sys
sys.path.insert(0, str(Path(__file__).parent.parent))
from utils.database_factory import get_database_service, get_database_type
from utils.metrics_optimizer import should_optimize_for_mongodb, optimize_metrics_for_storage


def load_raw_data(config) -> Dict[str, Any]:
    """
    Charge les donn√©es brutes depuis fichiers locaux
    (Sur EC2, les fichiers doivent √™tre upload√©s manuellement dans data/)
    
    Args:
        config: Configuration
    
    Returns:
        Dict avec toutes les donn√©es brutes par type
    """
    print("üìÅ Chargement des donn√©es depuis fichiers locaux...")
    return load_raw_data_local(config)


def load_raw_data_local(config) -> Dict[str, Any]:
    """
    Charge les donn√©es depuis fichiers locaux
    
    Args:
        config: Configuration
    
    Returns:
        Dict avec toutes les donn√©es brutes par type
    """
    raw_data = {
        "bikes": None,
        "traffic": None,
        "weather": None,
        "comptages": None,
        "chantiers": None,
        "referentiel": None
    }
    
    # Charger donn√©es API (JSON)
    try:
        # Bikes - Charger TOUS les fichiers et les combiner
        bikes_files = find_json_files(str(config.BIKES_JSON_PATH))
        if bikes_files:
            print(f"üìÅ Trouv√© {len(bikes_files)} fichier(s) bikes")
            if len(bikes_files) > 1:
                print(f"  ‚Üí Combinaison de {len(bikes_files)} fichiers...")
                raw_data["bikes"] = load_and_combine_json_files(bikes_files)
            else:
                raw_data["bikes"] = load_json(bikes_files[0])
        
        # Traffic - Charger TOUS les fichiers et les combiner
        traffic_files = find_json_files(str(config.TRAFFIC_JSON_PATH))
        if traffic_files:
            print(f"üìÅ Trouv√© {len(traffic_files)} fichier(s) traffic")
            if len(traffic_files) > 1:
                print(f"  ‚Üí Combinaison de {len(traffic_files)} fichiers...")
                raw_data["traffic"] = load_and_combine_json_files(traffic_files)
            else:
                raw_data["traffic"] = load_json(traffic_files[0])
        
        # Weather - Charger TOUS les fichiers et les combiner
        weather_files = find_json_files(str(config.WEATHER_JSON_PATH))
        if weather_files:
            print(f"üìÅ Trouv√© {len(weather_files)} fichier(s) weather")
            if len(weather_files) > 1:
                print(f"  ‚Üí Combinaison de {len(weather_files)} fichiers...")
                raw_data["weather"] = load_and_combine_json_files(weather_files)
            else:
                raw_data["weather"] = load_json(weather_files[0])
    except Exception as e:
        print(f"Erreur chargement donn√©es API: {e}")
    
    # Charger donn√©es Batch (CSV)
    try:
        # Comptages - Chercher tous les fichiers CSV dans le r√©pertoire
        comptages_dir = config.COMPTAGES_CSV.parent
        comptages_files = find_csv_files(str(comptages_dir), "comptages*.csv")
        if comptages_files:
            print(f"üìÅ Trouv√© {len(comptages_files)} fichier(s) comptages")
            if len(comptages_files) > 1:
                print(f"  ‚ö† Plusieurs fichiers trouv√©s, utilisation du premier: {comptages_files[0]}")
                print(f"  üí° Pour traiter plusieurs fichiers, utilisez le traitement par chunk")
            raw_data["comptages"] = comptages_files[0]  # Utiliser le premier pour compatibilit√©
        elif config.COMPTAGES_CSV.exists():
            raw_data["comptages"] = str(config.COMPTAGES_CSV)
        
        # Chantiers - Chercher tous les fichiers CSV
        chantiers_dir = config.CHANTIERS_CSV.parent
        chantiers_files = find_csv_files(str(chantiers_dir), "chantiers*.csv")
        if chantiers_files:
            print(f"üìÅ Trouv√© {len(chantiers_files)} fichier(s) chantiers")
            if len(chantiers_files) > 1:
                print(f"  ‚ö† Plusieurs fichiers trouv√©s, utilisation du premier: {chantiers_files[0]}")
            raw_data["chantiers"] = chantiers_files[0]
        elif config.CHANTIERS_CSV.exists():
            raw_data["chantiers"] = str(config.CHANTIERS_CSV)
        
        # R√©f√©rentiel - Chercher tous les fichiers CSV
        referentiel_dir = config.REFERENTIEL_CSV.parent
        referentiel_files = find_csv_files(str(referentiel_dir), "referentiel*.csv")
        if referentiel_files:
            print(f"üìÅ Trouv√© {len(referentiel_files)} fichier(s) r√©f√©rentiel")
            if len(referentiel_files) > 1:
                print(f"  ‚ö† Plusieurs fichiers trouv√©s, utilisation du premier: {referentiel_files[0]}")
            raw_data["referentiel"] = referentiel_files[0]
        elif config.REFERENTIEL_CSV.exists():
            raw_data["referentiel"] = str(config.REFERENTIEL_CSV)
    except Exception as e:
        print(f"Erreur chargement donn√©es batch: {e}")
    
    return raw_data


def load_raw_data_from_s3(config) -> Dict[str, Any]:
    """
    Charge les donn√©es directement depuis S3
    
    Args:
        config: Configuration
    
    Returns:
        Dict avec donn√©es par type
    """
    import os
    import tempfile
    from utils.aws_services import S3Service
    
    raw_data = {}
    
    # Configuration S3 EN DUR (plus besoin de .env)
    bucket_name = "bucket-cityflow-paris-s3-raw"
    prefix = "cityflow-raw/raw"
    
    print(f"üì¶ S3 Bucket: {bucket_name}")
    print(f"üì¶ S3 Prefix: {prefix}")
    
    s3_service = S3Service(bucket_name)
    
    try:
        # Charger donn√©es API (JSON/JSONL)
        # Bikes
        bikes_files = s3_service.list_files_in_s3(f"{prefix}/api/bikes/", ".jsonl")
        if not bikes_files:
            bikes_files = s3_service.list_files_in_s3(f"{prefix}/api/bikes/", ".json")
        
        if bikes_files:
            print(f"üìÅ Trouv√© {len(bikes_files)} fichier(s) bikes dans S3")
            bikes_data = []
            for file_key in bikes_files:
                data = s3_service.read_json_from_s3(file_key)
                if data:
                    bikes_data.append(data)
            
            # Combiner toutes les donn√©es
            if bikes_data:
                combined = {"data": []}
                for d in bikes_data:
                    if "data" in d:
                        if isinstance(d["data"], list):
                            combined["data"].extend(d["data"])
                        else:
                            combined["data"].append(d["data"])
                raw_data["bikes"] = combined
        
        # Traffic
        traffic_files = s3_service.list_files_in_s3(f"{prefix}/api/traffic/", ".jsonl")
        if not traffic_files:
            traffic_files = s3_service.list_files_in_s3(f"{prefix}/api/traffic/", ".json")
        
        if traffic_files:
            print(f"üìÅ Trouv√© {len(traffic_files)} fichier(s) traffic dans S3")
            # Combiner tous les fichiers traffic
            all_disruptions = []
            for file_key in traffic_files:
                data = s3_service.read_json_from_s3(file_key)
                if data and "data" in data:
                    items = data["data"] if isinstance(data["data"], list) else [data["data"]]
                    for item in items:
                        if "disruptions" in item:
                            all_disruptions.extend(item["disruptions"])
            
            if all_disruptions:
                raw_data["traffic"] = {"disruptions": all_disruptions}
        
        # Weather
        weather_files = s3_service.list_files_in_s3(f"{prefix}/api/weather/", ".jsonl")
        if not weather_files:
            weather_files = s3_service.list_files_in_s3(f"{prefix}/api/weather/", ".json")
        
        if weather_files:
            print(f"üìÅ Trouv√© {len(weather_files)} fichier(s) weather dans S3")
            data = s3_service.read_json_from_s3(weather_files[0])
            if data:
                raw_data["weather"] = data
        
        # Charger donn√©es BATCH (CSV) - T√©l√©chargement temporaire pour traitement
        # Les CSV sont trop volumineux pour √™tre charg√©s en m√©moire, on les t√©l√©charge temporairement
        
        # Comptages
        comptages_files = s3_service.list_files_in_s3(f"{prefix}/batch/", ".csv")
        comptages_files = [f for f in comptages_files if "comptages" in f.lower()]
        if comptages_files:
            print(f"üìÅ Trouv√© {len(comptages_files)} fichier(s) comptages dans S3 ({comptages_files[0]})")
            print(f"  ‚Üí T√©l√©chargement temporaire (fichier volumineux)...")
            with tempfile.NamedTemporaryFile(mode='w', suffix='.csv', delete=False) as tmp:
                content = s3_service.read_csv_from_s3(comptages_files[0])
                if content:
                    tmp.write(content)
                    tmp.flush()
                    raw_data["comptages"] = tmp.name
                    print(f"  ‚úÖ Fichier temporaire: {tmp.name}")
        
        # Chantiers
        chantiers_files = s3_service.list_files_in_s3(f"{prefix}/batch/", ".csv")
        chantiers_files = [f for f in chantiers_files if "chantiers" in f.lower()]
        if chantiers_files:
            print(f"üìÅ Trouv√© {len(chantiers_files)} fichier(s) chantiers dans S3")
            with tempfile.NamedTemporaryFile(mode='w', suffix='.csv', delete=False) as tmp:
                content = s3_service.read_csv_from_s3(chantiers_files[0])
                if content:
                    tmp.write(content)
                    tmp.flush()
                    raw_data["chantiers"] = tmp.name
        
        # R√©f√©rentiel
        referentiel_files = s3_service.list_files_in_s3(f"{prefix}/batch/", ".csv")
        referentiel_files = [f for f in referentiel_files if "referentiel" in f.lower() or "geographique" in f.lower()]
        if referentiel_files:
            print(f"üìÅ Trouv√© {len(referentiel_files)} fichier(s) r√©f√©rentiel dans S3")
            with tempfile.NamedTemporaryFile(mode='w', suffix='.csv', delete=False) as tmp:
                content = s3_service.read_csv_from_s3(referentiel_files[0])
                if content:
                    tmp.write(content)
                    tmp.flush()
                    raw_data["referentiel"] = tmp.name
    
    except Exception as e:
        print(f"‚ö† Erreur chargement depuis S3: {e}")
        import traceback
        traceback.print_exc()
    
    return raw_data


def initialize_processors(config) -> Dict[str, Any]:
    """
    Initialise tous les processeurs
    
    Args:
        config: Configuration
    
    Returns:
        Dict des processeurs par type
    """
    return {
        "bikes": BikesProcessor(config),
        "traffic": TrafficProcessor(config),
        "weather": WeatherProcessor(config),
        "comptages": ComptagesProcessor(config),
        "chantiers": ChantiersProcessor(config),
        "referentiel": ReferentielProcessor(config)
    }


def enrich_multi_source(results: Dict, referentiel_data: Optional[Dict] = None) -> Dict:
    """
    Enrichit les r√©sultats avec jointures multi-sources
    
    Args:
        results: R√©sultats de traitement
        referentiel_data: Donn√©es r√©f√©rentiel pour enrichissement
    
    Returns:
        R√©sultats enrichis
    """
    # Enrichir comptages avec r√©f√©rentiel
    # (Simplifi√© - dans un vrai projet, utiliser referentiel_data pour enrichir)
    if referentiel_data and "comptages" in results:
        # TODO: Utiliser referentiel_data pour enrichir les m√©triques comptages
        # avec libelles et m√©tadonn√©es g√©ographiques
        pass
    
    # Enrichir avec chantiers (intersection g√©ographique)
    # (Simplifi√© - n√©cessiterait calcul intersections g√©ographiques)
    
    return results


def cleanup_processed_chunks(config, keep_chunks=False):
    """
    Nettoie les fichiers chunks temporaires apr√®s traitement
    
    Args:
        config: Configuration
        keep_chunks: Si True, garde les chunks (pour debug)
    """
    if keep_chunks:
        return
    
    import glob
    import os
    
    chunk_files = glob.glob(str(config.PROCESSED_DIR / "*_chunk_*.csv"))
    
    if chunk_files:
        deleted_count = 0
        for chunk_file in chunk_files:
            try:
                os.remove(chunk_file)
                deleted_count += 1
            except Exception as e:
                print(f"  ‚ö† Erreur suppression {chunk_file}: {e}")
        
        if deleted_count > 0:
            print(f"  ‚úì {deleted_count} fichiers chunks nettoy√©s")


def export_results(results: Dict, config, date: Optional[str] = None) -> None:
    """
    Exporte les m√©triques calcul√©es vers la base de donn√©es (MongoDB ou DynamoDB)
    
    Args:
        results: R√©sultats de traitement
        config: Configuration
        date: Date au format YYYY-MM-DD (d√©faut: aujourd'hui)
    """
    # D√©terminer la date
    if date is None:
        date = datetime.now().strftime("%Y-%m-%d")
    
    # Remplir la date dans toutes les m√©triques
    for data_type, result in results.items():
        if result and result.get("indicators"):
            indicators = result.get("indicators", {})
            # Remplir date dans les m√©triques individuelles
            if "metrics" in indicators and isinstance(indicators["metrics"], list):
                for metric in indicators["metrics"]:
                    # V√©rifier que metric est bien un dict
                    if isinstance(metric, dict) and "date" in metric and metric["date"] == "":
                        metric["date"] = date
            
            # Remplir date dans les top 10 tron√ßons
            if "top_10_troncons" in indicators:
                for troncon in indicators["top_10_troncons"]:
                    if "date" in troncon and troncon["date"] == "":
                        troncon["date"] = date
            
            # Remplir date dans les top 10 zones congestionn√©es
            if "top_10_zones_congestionnees" in indicators:
                for zone in indicators["top_10_zones_congestionnees"]:
                    if "date" in zone and zone["date"] == "":
                        zone["date"] = date
                    # S'assurer que zone_fallback est pr√©sent
                    if "zone_fallback" not in zone or not zone.get("zone_fallback"):
                        arr = zone.get("arrondissement", "Unknown")
                        if arr != "Unknown":
                            zone["zone_fallback"] = f"Arrondissement {arr}"
                        else:
                            zone["zone_fallback"] = "Unknown"
            
            # Remplir date dans les alertes de congestion
            if "alertes_congestion" in indicators:
                for alerte in indicators["alertes_congestion"]:
                    if "date" in alerte and alerte["date"] == "":
                        alerte["date"] = date
    
    # Obtenir le service de base de donn√©es (MongoDB ou DynamoDB selon config)
    try:
        db_service = get_database_service()
        db_type = get_database_type()
    except Exception as e:
        print(f"\n‚úó Erreur initialisation base de donn√©es: {e}")
        print("üí° Les m√©triques seront sauvegard√©es en local uniquement")
        db_service = None
        db_type = "local"
    
    # Exporter m√©triques par type
    exported_count = 0
    for data_type, result in results.items():
        if result and result.get("success"):
            indicators = result.get("indicators", {})
            if indicators:
                # V√©rifier si optimisation n√©cessaire pour MongoDB
                if db_service and should_optimize_for_mongodb(data_type, indicators):
                    # Cr√©er version optimis√©e pour MongoDB (sans liste compl√®te des tron√ßons)
                    optimized_indicators = optimize_metrics_for_storage(data_type, indicators)
                    print(f"  ‚ö† M√©triques {data_type} optimis√©es pour stockage (taille r√©duite)")
                    print(f"     ‚Üí Version compl√®te disponible en fichier local uniquement")
                    
                    # Sauvegarder version optimis√©e dans la base de donn√©es
                    success = db_service.save_metrics(
                        metrics=optimized_indicators,
                        data_type=data_type,
                        date=date
                    )
                    if success:
                        exported_count += 1
                        print(f"‚úì M√©triques {data_type} (summary) export√©es vers {db_type.upper()}")
                    else:
                        print(f"‚úó Erreur export m√©triques {data_type} vers {db_type.upper()}")
                else:
                    # Sauvegarder version compl√®te dans la base de donn√©es
                    if db_service:
                        success = db_service.save_metrics(
                            metrics=indicators,
                            data_type=data_type,
                            date=date
                        )
                        if success:
                            exported_count += 1
                            print(f"‚úì M√©triques {data_type} export√©es vers {db_type.upper()}")
                        else:
                            print(f"‚úó Erreur export m√©triques {data_type} vers {db_type.upper()}")
                
                # Toujours sauvegarder version compl√®te en local (backup + r√©f√©rence)
                if not os.getenv("AWS_EXECUTION_ENV"):
                    from processors.utils.file_utils import save_json
                    output_path = config.METRICS_DIR / f"{data_type}_metrics_{date}.json"
                    save_json(indicators, str(output_path))
                    print(f"  ‚Üí Sauvegarde locale (backup complet): {output_path}")
    
    # Fermer connexion MongoDB si applicable
    if db_service and hasattr(db_service, 'close'):
        db_service.close()
    
    # Nettoyer chunks temporaires apr√®s export r√©ussi
    cleanup_processed_chunks(config, keep_chunks=False)
    
    print(f"\n‚úì {exported_count} types de m√©triques export√©s vers {db_type.upper()}")
    print("\nüí° Pour g√©n√©rer le rapport quotidien (instance s√©par√©e), ex√©cutez:")
    print(f"   python report_generator/main.py {date}")


def main(date: Optional[str] = None):
    """
    Point d'entr√©e principal
    
    Args:
        date: Date au format YYYY-MM-DD (d√©faut: aujourd'hui)
    """
    # D√©terminer la date de traitement
    if date is None:
        date = datetime.now().strftime("%Y-%m-%d")
    else:
        # Valider le format de date
        try:
            datetime.strptime(date, "%Y-%m-%d")
        except ValueError:
            print(f"‚ö† Format de date invalide: {date}, utilisation de la date d'aujourd'hui")
            date = datetime.now().strftime("%Y-%m-%d")
    
    print("=" * 60)
    print("CityFlow Analytics - Traitement des Donn√©es")
    print(f"Date: {date}")
    print("=" * 60)
    
    try:
        # 1. Chargement configuration
        print("\n[1/6] Chargement configuration...")
        config = settings
        print("‚úì Configuration charg√©e")
        
        # 2. Initialisation processeurs
        print("\n[2/6] Initialisation processeurs...")
        processors = initialize_processors(config)
        print(f"‚úì {len(processors)} processeurs initialis√©s")
        
        # 3. Chargement donn√©es brutes
        print("\n[3/6] Chargement donn√©es brutes...")
        raw_data = load_raw_data(config)
        
        data_loaded = sum(1 for v in raw_data.values() if v is not None)
        print(f"‚úì {data_loaded} sources de donn√©es charg√©es")
        
        # 4. Traitement par type de donn√©es
        print("\n[4/6] Traitement des donn√©es...")
        results = {}
        
        # Traiter r√©f√©rentiel en premier (pour enrichissement)
        if raw_data.get("referentiel"):
            print("  ‚Üí Traitement r√©f√©rentiel g√©ographique...")
            results["referentiel"] = processors["referentiel"].process(raw_data["referentiel"])
        
        # Traiter autres donn√©es
        for data_type, processor in processors.items():
            if data_type == "referentiel":
                continue  # D√©j√† trait√©
            
            data = raw_data.get(data_type)
            if data is None:
                print(f"  ‚ö† Pas de donn√©es pour {data_type}")
                continue
            
            print(f"  ‚Üí Traitement {data_type}...")
            
            try:
                # Cas sp√©cial pour comptages (gros fichier)
                if data_type == "comptages" and isinstance(data, str):
                    result = processors[data_type].process_large_file(data)
                else:
                    result = processor.process(data)
                
                results[data_type] = result
                print(f"    ‚úì {data_type} trait√© avec succ√®s")
            except Exception as e:
                print(f"    ‚úó Erreur traitement {data_type}: {e}")
                results[data_type] = {"success": False, "errors": [str(e)]}
        
        # 5. Enrichissement multi-sources
        print("\n[5/6] Enrichissement multi-sources...")
        referentiel_data = results.get("referentiel")
        results = enrich_multi_source(results, referentiel_data)
        print("‚úì Enrichissement termin√©")
        
        # 6. Export r√©sultats (m√©triques uniquement)
        print("\n[6/6] Export des m√©triques...")
        export_results(results, config, date=date)
        print("‚úì Export termin√©")
        
        print("\n" + "=" * 60)
        print("Traitement termin√© avec succ√®s!")
        print("=" * 60)
        db_type = get_database_type()
        print(f"\nüìä M√©triques export√©es dans {db_type.upper()}")
        print("üìã Pour g√©n√©rer le rapport (instance s√©par√©e), ex√©cutez:")
        print("   python report_generator/main.py")
        print("=" * 60)
        
        return results
    
    except Exception as e:
        print(f"\n‚úó ERREUR FATALE: {e}")
        import traceback
        traceback.print_exc()
        return None


if __name__ == "__main__":
    results = main()

